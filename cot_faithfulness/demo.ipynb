{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CoT Faithfulness Evaluation Demo\n",
    "\n",
    "This notebook demonstrates how to use the `cot_faithfulness` module to evaluate\n",
    "Chain-of-Thought (CoT) faithfulness in language models.\n",
    "\n",
    "## What is CoT Faithfulness?\n",
    "\n",
    "When LLMs use step-by-step reasoning (Chain-of-Thought), we want to know:\n",
    "- **Is the reasoning actually driving the answer?** (Faithful)\n",
    "- **Or is it just post-hoc rationalization?** (Unfaithful)\n",
    "\n",
    "This matters for interpretability because unfaithful CoT means we can't trust\n",
    "the model's explanations of its own reasoning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, install dependencies and set up your API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add parent directory to path (if running from this folder)\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "# Import the evaluator\n",
    "from cot_faithfulness import CoTFaithfulnessEvaluator\n",
    "\n",
    "print(\"CoT Faithfulness Evaluator loaded!\")\n",
    "print(\"Available datasets:\", CoTFaithfulnessEvaluator.available_datasets())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Define Your Model\n",
    "\n",
    "You need to provide a function that takes a prompt string and returns\n",
    "the model's response string. Here's an example using Together AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "# Set up your API client (example using Together AI)\n",
    "client = OpenAI(\n",
    "    api_key=os.environ.get(\"TOGETHER_API_KEY\"),\n",
    "    base_url=\"https://api.together.xyz/v1\"\n",
    ")\n",
    "\n",
    "# Define the model you want to test\n",
    "MODEL_ID = \"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\"\n",
    "MODEL_NAME = \"Llama-3.1-8B\"  # Display name\n",
    "\n",
    "def my_model(prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Your model's generate function.\n",
    "    Takes a prompt, returns the response text.\n",
    "    \"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL_ID,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=300,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Quick test\n",
    "print(\"Testing API connection...\")\n",
    "print(my_model(\"Say 'Hello!' if you can hear me.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Run Faithfulness Evaluation\n",
    "\n",
    "The evaluator runs two tests:\n",
    "1. **Truncation Test**: Cuts off reasoning halfway - if answer changes, reasoning matters\n",
    "2. **Error Injection Test**: Injects wrong reasoning - if model follows, it reads the reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create evaluator\n",
    "evaluator = CoTFaithfulnessEvaluator()\n",
    "\n",
    "# See available options\n",
    "print(\"Available datasets:\", evaluator.available_datasets())\n",
    "print(\"Available baselines:\", evaluator.available_baselines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation (this will make API calls)\n",
    "results = evaluator.evaluate(\n",
    "    generate_fn=my_model,\n",
    "    model_name=MODEL_NAME,\n",
    "    dataset=\"arc\",      # Options: 'arc', 'aqua', 'mmlu'\n",
    "    n_samples=10,       # Number of questions to test\n",
    "    verbose=True,       # Show progress\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Compare to Baselines\n",
    "\n",
    "Compare your model's faithfulness against pre-computed baselines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare against Llama-3.1-70B baseline\n",
    "comparison = evaluator.compare_to_baseline(results, baseline=\"Llama-3.1-70B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the comparison\n",
    "evaluator.plot_comparison(results, baseline=\"Llama-3.1-70B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Analyze Detailed Results\n",
    "\n",
    "Dig into specific question results to understand the patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# View truncation test details\n",
    "trunc_df = pd.DataFrame(results.truncation_details)\n",
    "print(\"Truncation Test Results:\")\n",
    "print(f\"  Changed answer: {trunc_df['changed'].sum()}/{len(trunc_df)}\")\n",
    "display(trunc_df)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# View error injection details  \n",
    "error_df = pd.DataFrame(results.error_details)\n",
    "print(\"\\nError Injection Results:\")\n",
    "print(f\"  Followed error: {error_df['followed_error'].sum()}/{len(error_df)}\")\n",
    "display(error_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation Guide\n",
    "\n",
    "### What the scores mean:\n",
    "\n",
    "| Score | Truncation Test | Error Injection |\n",
    "|-------|-----------------|------------------|\n",
    "| **High (>70%)** | Model relies on its reasoning | Model reads its reasoning |\n",
    "| **Medium (40-70%)** | Partial reliance on reasoning | Sometimes reads reasoning |\n",
    "| **Low (<40%)** | Answer independent of reasoning | Ignores provided reasoning |\n",
    "\n",
    "### Key findings from research:\n",
    "- Larger models are not necessarily more faithful\n",
    "- Math/logic tasks tend to show higher faithfulness\n",
    "- Simple knowledge questions often show lower faithfulness\n",
    "\n",
    "### What this means for interpretability:\n",
    "- **High faithfulness** → You can trust the model's CoT explanations\n",
    "- **Low faithfulness** → The model may be using shortcuts, not the stated reasoning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced: Test Multiple Models\n",
    "\n",
    "Compare faithfulness across different model sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Testing multiple models\n",
    "models_to_test = [\n",
    "    (\"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\", \"Llama-8B\"),\n",
    "    # Add more models as needed\n",
    "]\n",
    "\n",
    "all_results = {}\n",
    "\n",
    "for model_id, model_name in models_to_test:\n",
    "    def generate(prompt):\n",
    "        resp = client.chat.completions.create(\n",
    "            model=model_id,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            max_tokens=300,\n",
    "            temperature=0.7,\n",
    "        )\n",
    "        return resp.choices[0].message.content\n",
    "    \n",
    "    result = evaluator.evaluate(\n",
    "        generate_fn=generate,\n",
    "        model_name=model_name,\n",
    "        dataset=\"arc\",\n",
    "        n_samples=10,\n",
    "    )\n",
    "    all_results[model_name] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all tested models\n",
    "comparison_df = pd.DataFrame([\n",
    "    {\n",
    "        \"Model\": r.model_name,\n",
    "        \"Truncation %\": r.truncation_faithfulness,\n",
    "        \"Error Following %\": r.error_following,\n",
    "        \"Avg Faithfulness %\": r.avg_faithfulness,\n",
    "    }\n",
    "    for r in all_results.values()\n",
    "])\n",
    "\n",
    "print(\"\\nFaithfulness Comparison:\")\n",
    "display(comparison_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Results\n",
    "\n",
    "Save your results for later analysis or sharing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Export to JSON\n",
    "with open(f\"{MODEL_NAME}_faithfulness_results.json\", \"w\") as f:\n",
    "    json.dump(results.to_dict(), f, indent=2)\n",
    "\n",
    "print(f\"Results saved to {MODEL_NAME}_faithfulness_results.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
