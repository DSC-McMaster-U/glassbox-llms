"""
Baseline faithfulness results for comparison.

These baselines were generated by running the CoT faithfulness tests
on various models using the Together AI API. Results are stored here
for users to compare their own models against.

Test conditions:
- 20 samples per dataset
- Temperature: 0.7 for CoT generation, 0.3 for answer extraction
- Truncation ratio: 0.5 (first half of reasoning)
"""

from typing import Dict, Any, Optional, List


# Baseline results from experiments
# Format: {model_name: {dataset_name: {metrics}}}
BASELINES: Dict[str, Dict[str, Dict[str, float]]] = {
    # Results from notebook experiments (20 samples, ARC-Easy validation set)
    "Gemma-3n-E4B": {
        "arc": {
            "truncation_faithfulness": 65.0,
            "error_following": 20.0,
            "avg_faithfulness": 42.5,
            "n_samples": 20,
        },
    },
    "Llama-3.1-70B": {
        "arc": {
            "truncation_faithfulness": 95.0,
            "error_following": 40.0,
            "avg_faithfulness": 67.5,
            "n_samples": 20,
        },
        "aqua": {
            "truncation_faithfulness": 85.0,
            "error_following": 45.0,
            "avg_faithfulness": 65.0,
            "n_samples": 20,
        },
        "mmlu": {
            "truncation_faithfulness": 80.0,
            "error_following": 35.0,
            "avg_faithfulness": 57.5,
            "n_samples": 20,
        },
    },
    "Qwen-3-235B": {
        "arc": {
            "truncation_faithfulness": 60.0,
            "error_following": 5.0,
            "avg_faithfulness": 32.5,
            "n_samples": 20,
        },
    },
}

# Model metadata for context
MODEL_INFO: Dict[str, Dict[str, Any]] = {
    "Gemma-3n-E4B": {
        "full_name": "google/gemma-3n-E4B-it",
        "size": "4B",
        "family": "Gemma",
        "provider": "Google",
    },
    "Llama-3.1-70B": {
        "full_name": "meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo",
        "size": "70B", 
        "family": "Llama",
        "provider": "Meta",
    },
    "Qwen-3-235B": {
        "full_name": "Qwen/Qwen3-235B-A22B-fp8-tput",
        "size": "235B",
        "family": "Qwen",
        "provider": "Alibaba",
    },
}


def get_baseline(
    model_name: str, 
    dataset_name: str
) -> Optional[Dict[str, float]]:
    """
    Get baseline results for a specific model and dataset.
    
    Args:
        model_name: Name of the baseline model
        dataset_name: Name of the dataset
        
    Returns:
        Dict with faithfulness metrics, or None if not found
    """
    if model_name not in BASELINES:
        return None
    if dataset_name not in BASELINES[model_name]:
        return None
    return BASELINES[model_name][dataset_name]


def list_baselines() -> Dict[str, List[str]]:
    """
    List all available baselines.
    
    Returns:
        Dict mapping model names to list of datasets they have results for
    """
    return {
        model: list(datasets.keys()) 
        for model, datasets in BASELINES.items()
    }


def get_model_info(model_name: str) -> Optional[Dict[str, Any]]:
    """Get metadata about a baseline model."""
    return MODEL_INFO.get(model_name)


def add_baseline(
    model_name: str,
    dataset_name: str,
    results: Dict[str, float],
    model_info: Optional[Dict[str, Any]] = None,
) -> None:
    """
    Add a new baseline result (for extending the library).
    
    Args:
        model_name: Name for the model
        dataset_name: Dataset tested on
        results: Dict with truncation_faithfulness, error_following, avg_faithfulness
        model_info: Optional metadata about the model
    """
    if model_name not in BASELINES:
        BASELINES[model_name] = {}
    
    BASELINES[model_name][dataset_name] = results
    
    if model_info:
        MODEL_INFO[model_name] = model_info
