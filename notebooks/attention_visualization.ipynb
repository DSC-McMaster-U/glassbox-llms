{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21f851a0",
   "metadata": {},
   "source": [
    "# Attention Visualization in Transformer Models\n",
    "\n",
    "## Objective\n",
    "This notebook demonstrates how to visualize and interpret attention mechanisms in transformer models. We'll:\n",
    "\n",
    "1. Load a pre-trained model (DistilBERT)\n",
    "2. Extract attention weights for sample text\n",
    "3. Create various visualizations of attention patterns\n",
    "4. Analyze what the model is focusing on\n",
    "\n",
    "## Key Concepts\n",
    "- **Attention Mechanism**: How models decide which parts of the input to focus on\n",
    "- **Multi-Head Attention**: Multiple attention patterns learned in parallel\n",
    "- **Attention Heads**: Individual attention mechanisms that may capture different linguistic patterns\n",
    "- **Attention Maps**: Visualizations showing which tokens attend to which other tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e456e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install transformers torch matplotlib seaborn bertviz ipywidgets\n",
    "!pip install datasets  # For additional text examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa45f1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "from bertviz import head_view, model_view\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"GPU available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30c756d",
   "metadata": {},
   "source": [
    "## 1. Model Setup\n",
    "\n",
    "We'll use **DistilBERT**, a smaller and faster version of BERT that retains most of its performance while being more efficient for our visualization purposes.\n",
    "\n",
    "### Why DistilBERT?\n",
    "- **Lightweight**: 6 layers vs BERT's 12 layers\n",
    "- **Fast inference**: Good for interactive exploration\n",
    "- **Clear attention patterns**: Well-documented attention behaviors\n",
    "- **Good for visualization**: Manageable number of attention heads (12 heads per layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607d6db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load DistilBERT model and tokenizer\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "\n",
    "print(\"Loading model and tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "config = AutoConfig.from_pretrained(model_name, output_attentions=True)\n",
    "model = AutoModel.from_pretrained(model_name, config=config)\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model loaded: {model_name}\")\n",
    "print(f\"Number of layers: {config.n_layers}\")\n",
    "print(f\"Number of attention heads per layer: {config.n_heads}\")\n",
    "print(f\"Hidden size: {config.dim}\")\n",
    "print(f\"Vocabulary size: {config.vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fbcf7f",
   "metadata": {},
   "source": [
    "## 2. Text Processing and Attention Extraction\n",
    "\n",
    "Let's define some sample texts to analyze and create functions to extract attention weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66dd0458",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_attention(text, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Extract attention weights for a given text.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text to analyze\n",
    "        model: Pre-trained transformer model\n",
    "        tokenizer: Corresponding tokenizer\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (tokens, attention_weights, input_ids)\n",
    "    \"\"\"\n",
    "    # Tokenize the input\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", add_special_tokens=True)\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "    \n",
    "    # Get model outputs with attention\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Extract attention weights\n",
    "    # Shape: (num_layers, batch_size, num_heads, seq_len, seq_len)\n",
    "    attention_weights = outputs.attentions\n",
    "    \n",
    "    return tokens, attention_weights, inputs[\"input_ids\"]\n",
    "\n",
    "# Test the function with sample text\n",
    "sample_text = \"The cat sat on the mat.\"\n",
    "tokens, attention_weights, input_ids = extract_attention(sample_text, model, tokenizer)\n",
    "\n",
    "print(f\"Sample text: '{sample_text}'\")\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print(f\"Number of layers: {len(attention_weights)}\")\n",
    "print(f\"Attention shape per layer: {attention_weights[0].shape}\")\n",
    "print(f\"(batch_size, num_heads, seq_len, seq_len)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dab0b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention_head(attention_weights, tokens, layer_idx, head_idx, figsize=(10, 8)):\n",
    "    \"\"\"\n",
    "    Plot attention weights for a specific layer and head.\n",
    "    \n",
    "    Args:\n",
    "        attention_weights: Attention weights from model\n",
    "        tokens: List of tokens\n",
    "        layer_idx: Which layer to visualize\n",
    "        head_idx: Which attention head to visualize\n",
    "        figsize: Figure size for the plot\n",
    "    \"\"\"\n",
    "    # Extract attention for specific layer and head\n",
    "    # Shape: (seq_len, seq_len)\n",
    "    attention = attention_weights[layer_idx][0, head_idx].numpy()\n",
    "    \n",
    "    plt.figure(figsize=figsize)\n",
    "    \n",
    "    # Create heatmap\n",
    "    sns.heatmap(\n",
    "        attention,\n",
    "        xticklabels=tokens,\n",
    "        yticklabels=tokens,\n",
    "        cmap='Blues',\n",
    "        annot=True,\n",
    "        fmt='.2f',\n",
    "        square=True,\n",
    "        cbar_kws={'label': 'Attention Weight'}\n",
    "    )\n",
    "    \n",
    "    plt.title(f'Attention Head {head_idx}, Layer {layer_idx}')\n",
    "    plt.xlabel('Keys (attending to)')\n",
    "    plt.ylabel('Queries (attending from)')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_layer_attention_summary(attention_weights, tokens, layer_idx, figsize=(12, 8)):\n",
    "    \"\"\"\n",
    "    Plot average attention across all heads for a specific layer.\n",
    "    \"\"\"\n",
    "    # Average across all heads for this layer\n",
    "    layer_attention = attention_weights[layer_idx][0].mean(dim=0).numpy()\n",
    "    \n",
    "    plt.figure(figsize=figsize)\n",
    "    \n",
    "    sns.heatmap(\n",
    "        layer_attention,\n",
    "        xticklabels=tokens,\n",
    "        yticklabels=tokens,\n",
    "        cmap='Reds',\n",
    "        annot=True,\n",
    "        fmt='.2f',\n",
    "        square=True,\n",
    "        cbar_kws={'label': 'Average Attention Weight'}\n",
    "    )\n",
    "    \n",
    "    plt.title(f'Layer {layer_idx} - Average Attention Across All Heads')\n",
    "    plt.xlabel('Keys (attending to)')\n",
    "    plt.ylabel('Queries (attending from)')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1064876d",
   "metadata": {},
   "source": [
    "## 3. Basic Attention Visualization\n",
    "\n",
    "Let's start with visualizing attention patterns for our sample text: \"The cat sat on the mat.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a275e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize attention for specific heads\n",
    "print(\"Visualizing different attention heads...\")\n",
    "\n",
    "# Plot attention for the first layer, first head\n",
    "plot_attention_head(attention_weights, tokens, layer_idx=0, head_idx=0)\n",
    "\n",
    "# Plot attention for a middle layer, different head\n",
    "plot_attention_head(attention_weights, tokens, layer_idx=2, head_idx=5)\n",
    "\n",
    "# Plot attention for the last layer, last head\n",
    "plot_attention_head(attention_weights, tokens, layer_idx=5, head_idx=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156d3cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize average attention per layer\n",
    "print(\"\\\\nVisualizing average attention across all heads per layer...\")\n",
    "\n",
    "# Plot average attention for first, middle, and last layers\n",
    "for layer_idx in [0, 2, 5]:\n",
    "    plot_layer_attention_summary(attention_weights, tokens, layer_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a34710",
   "metadata": {},
   "source": [
    "## 4. Interactive Attention Visualization with BertViz\n",
    "\n",
    "BertViz provides interactive visualizations that allow you to explore attention patterns more intuitively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88261c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive visualization with BertViz\n",
    "from bertviz import head_view, model_view\n",
    "\n",
    "# Prepare inputs for BertViz\n",
    "text = \"The cat sat on the mat.\"\n",
    "inputs = tokenizer.encode_plus(text, return_tensors='pt', add_special_tokens=True)\n",
    "input_ids = inputs['input_ids']\n",
    "attention = model(input_ids)[-1]  # Get attention weights\n",
    "\n",
    "# Convert to format expected by BertViz\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "\n",
    "print(\"Generating interactive attention visualization...\")\n",
    "print(\"Note: The visualization will appear below. Click on different heads and layers to explore!\")\n",
    "\n",
    "# Head view - shows attention for individual heads\n",
    "head_view(attention, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ddfc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model view - shows attention across all layers and heads\n",
    "print(\"\\\\nModel view - Overview of all layers and heads:\")\n",
    "model_view(attention, tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ad5530",
   "metadata": {},
   "source": [
    "## 5. Analyzing Different Text Types\n",
    "\n",
    "Let's explore how attention patterns change with different types of text and linguistic structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c63848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define different types of sentences to analyze\n",
    "test_sentences = [\n",
    "    \"The cat sat on the mat.\",                    # Simple sentence\n",
    "    \"The big red cat sat on the soft mat.\",      # Sentence with adjectives\n",
    "    \"The cat that I love sat on the mat.\",       # Sentence with relative clause\n",
    "    \"When the cat sat down, the mat moved.\",     # Complex sentence with subordinate clause\n",
    "    \"The cat and the dog played together.\",      # Sentence with coordination\n",
    "    \"She said that the cat was sleeping.\"        # Sentence with reported speech\n",
    "]\n",
    "\n",
    "def analyze_sentence_attention(sentence, layer_idx=2, head_idx=5):\n",
    "    \"\"\"Analyze attention for a sentence and provide insights.\"\"\"\n",
    "    tokens, attention_weights, _ = extract_attention(sentence, model, tokenizer)\n",
    "    \n",
    "    print(f\"\\\\n{'='*60}\")\n",
    "    print(f\"Analyzing: '{sentence}'\")\n",
    "    print(f\"Tokens: {tokens}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Plot attention for specified layer and head\n",
    "    plot_attention_head(attention_weights, tokens, layer_idx, head_idx, figsize=(12, 10))\n",
    "    \n",
    "    # Extract attention matrix for analysis\n",
    "    attention_matrix = attention_weights[layer_idx][0, head_idx].numpy()\n",
    "    \n",
    "    # Find highest attention connections\n",
    "    print(\"\\\\nTop 5 attention connections:\")\n",
    "    for i in range(len(tokens)):\n",
    "        for j in range(len(tokens)):\n",
    "            if i != j:  # Skip self-attention\n",
    "                print(f\"{tokens[i]} -> {tokens[j]}: {attention_matrix[i,j]:.3f}\")\n",
    "    \n",
    "    # Sort and show top connections\n",
    "    connections = []\n",
    "    for i in range(len(tokens)):\n",
    "        for j in range(len(tokens)):\n",
    "            if i != j:\n",
    "                connections.append((tokens[i], tokens[j], attention_matrix[i,j]))\n",
    "    \n",
    "    connections.sort(key=lambda x: x[2], reverse=True)\n",
    "    print(\"\\\\nStrongest attention connections:\")\n",
    "    for src, tgt, weight in connections[:5]:\n",
    "        print(f\"  {src} -> {tgt}: {weight:.3f}\")\n",
    "\n",
    "# Analyze the first few sentences\n",
    "for sentence in test_sentences[:3]:\n",
    "    analyze_sentence_attention(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e849b925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compare attention patterns across layers\n",
    "def compare_layers_attention(sentence, head_idx=5):\n",
    "    \"\"\"Compare attention patterns across different layers for the same sentence.\"\"\"\n",
    "    tokens, attention_weights, _ = extract_attention(sentence, model, tokenizer)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for layer_idx in range(6):  # DistilBERT has 6 layers\n",
    "        attention = attention_weights[layer_idx][0, head_idx].numpy()\n",
    "        \n",
    "        sns.heatmap(\n",
    "            attention,\n",
    "            xticklabels=tokens,\n",
    "            yticklabels=tokens,\n",
    "            cmap='Blues',\n",
    "            annot=True,\n",
    "            fmt='.2f',\n",
    "            square=True,\n",
    "            ax=axes[layer_idx],\n",
    "            cbar=False\n",
    "        )\n",
    "        \n",
    "        axes[layer_idx].set_title(f'Layer {layer_idx}')\n",
    "        axes[layer_idx].set_xlabel('Keys')\n",
    "        axes[layer_idx].set_ylabel('Queries')\n",
    "        axes[layer_idx].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.suptitle(f'Attention Patterns Across Layers\\\\nSentence: \"{sentence}\"\\\\nHead: {head_idx}', \n",
    "                 fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Compare layers for a complex sentence\n",
    "complex_sentence = \"The cat that I love sat on the mat.\"\n",
    "compare_layers_attention(complex_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec12bbd9",
   "metadata": {},
   "source": [
    "## 6. Attention Pattern Analysis & Observations\n",
    "\n",
    "Based on the visualizations above, let's document key observations about what attention appears to be focusing on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3ddb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Systematic analysis of attention patterns\n",
    "def analyze_attention_patterns(sentence):\n",
    "    \"\"\"\n",
    "    Systematically analyze and categorize attention patterns in a sentence.\n",
    "    \"\"\"\n",
    "    tokens, attention_weights, _ = extract_attention(sentence, model, tokenizer)\n",
    "    print(f\"\\\\nAnalyzing patterns in: '{sentence}'\")\n",
    "    print(f\"Tokens: {tokens}\")\n",
    "    \n",
    "    patterns = {\n",
    "        'positional': [],      # Attention to adjacent positions\n",
    "        'syntactic': [],       # Attention following syntactic structure\n",
    "        'semantic': [],        # Attention between semantically related words\n",
    "        'special_tokens': []   # Attention involving [CLS], [SEP]\n",
    "    }\n",
    "    \n",
    "    # Analyze each layer\n",
    "    for layer_idx in range(len(attention_weights)):\n",
    "        layer_attention = attention_weights[layer_idx][0].mean(dim=0).numpy()  # Average across heads\n",
    "        \n",
    "        # Find strong attention connections (threshold = 0.1)\n",
    "        strong_connections = []\n",
    "        for i in range(len(tokens)):\n",
    "            for j in range(len(tokens)):\n",
    "                if layer_attention[i, j] > 0.1 and i != j:\n",
    "                    strong_connections.append((i, j, tokens[i], tokens[j], layer_attention[i, j]))\n",
    "        \n",
    "        print(f\"\\\\nLayer {layer_idx} - Strong connections (>0.1):\")\n",
    "        for i, j, token_i, token_j, weight in strong_connections:\n",
    "            print(f\"  {token_i} -> {token_j}: {weight:.3f}\")\n",
    "            \n",
    "            # Categorize the connection\n",
    "            if abs(i - j) == 1:\n",
    "                patterns['positional'].append((layer_idx, token_i, token_j, weight))\n",
    "            elif token_i in ['[CLS]', '[SEP]'] or token_j in ['[CLS]', '[SEP]']:\n",
    "                patterns['special_tokens'].append((layer_idx, token_i, token_j, weight))\n",
    "            elif token_i in ['the', 'a', 'an'] and token_j not in ['[CLS]', '[SEP]']:\n",
    "                patterns['syntactic'].append((layer_idx, token_i, token_j, weight))\n",
    "            else:\n",
    "                patterns['semantic'].append((layer_idx, token_i, token_j, weight))\n",
    "    \n",
    "    return patterns\n",
    "\n",
    "# Analyze patterns for different sentence types\n",
    "sentences_to_analyze = [\n",
    "    \"The cat sat on the mat.\",\n",
    "    \"The big red cat sat on the soft mat.\",\n",
    "    \"The cat that I love sat on the mat.\"\n",
    "]\n",
    "\n",
    "all_patterns = {}\n",
    "for sentence in sentences_to_analyze:\n",
    "    all_patterns[sentence] = analyze_attention_patterns(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd3ecb3",
   "metadata": {},
   "source": [
    "## 7. Key Insights and Observations\n",
    "\n",
    "Based on our attention visualizations, here are the key patterns we typically observe in transformer attention:\n",
    "\n",
    "### 📍 **Positional Patterns**\n",
    "- **Adjacent Token Attention**: Many heads attend to immediately neighboring tokens\n",
    "- **Distance Decay**: Attention generally decreases with token distance\n",
    "- **Position-specific Behavior**: Different layers show different positional preferences\n",
    "\n",
    "### 🔤 **Syntactic Patterns**\n",
    "- **Determiner-Noun Relationships**: \"the\", \"a\", \"an\" often attend strongly to the nouns they modify\n",
    "- **Subject-Verb Connections**: Subjects and their corresponding verbs show strong attention\n",
    "- **Modifier Relationships**: Adjectives attend to the nouns they modify\n",
    "\n",
    "### 💭 **Semantic Patterns**\n",
    "- **Coreference**: Pronouns attend to their antecedents\n",
    "- **Semantic Similarity**: Words with related meanings show mutual attention\n",
    "- **Thematic Roles**: Arguments of verbs (subject, object) show structured attention patterns\n",
    "\n",
    "### 🎯 **Special Token Behavior**\n",
    "- **[CLS] Token**: Often acts as a \"summary\" token, attending broadly across the sentence\n",
    "- **[SEP] Token**: Boundary marker with specific attention patterns\n",
    "- **Sentence-level Information**: Special tokens help aggregate sentence-level representations\n",
    "\n",
    "### 🏗️ **Layer-wise Evolution**\n",
    "- **Early Layers**: Focus more on local, syntactic relationships\n",
    "- **Middle Layers**: Capture more complex syntactic structures\n",
    "- **Later Layers**: Encode higher-level semantic and discourse relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb48651",
   "metadata": {},
   "source": [
    "## 8. Explore Your Own Examples\n",
    "\n",
    "Use the space below to experiment with your own sentences and observe attention patterns!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f4a39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try your own sentences!\n",
    "# Modify the sentence below and run the cell to see attention patterns\n",
    "\n",
    "your_sentence = \"Your custom sentence goes here.\"\n",
    "\n",
    "# Quick analysis function\n",
    "def quick_attention_analysis(sentence, layer=2, head=5):\n",
    "    \"\"\"Quick visualization and analysis of a sentence.\"\"\"\n",
    "    tokens, attention_weights, _ = extract_attention(sentence, model, tokenizer)\n",
    "    \n",
    "    print(f\"Analyzing: '{sentence}'\")\n",
    "    print(f\"Tokens: {tokens}\")\n",
    "    \n",
    "    # Plot the attention\n",
    "    plot_attention_head(attention_weights, tokens, layer, head)\n",
    "    \n",
    "    # Show BertViz visualization\n",
    "    inputs = tokenizer.encode_plus(sentence, return_tensors='pt', add_special_tokens=True)\n",
    "    attention = model(inputs['input_ids'])[-1]\n",
    "    tokens_bertviz = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "    head_view(attention, tokens_bertviz)\n",
    "\n",
    "# Uncomment and modify the line below to try your own sentence:\n",
    "# quick_attention_analysis(\"The dog chased the cat up the tree.\")\n",
    "\n",
    "print(\"Ready for exploration! Modify 'your_sentence' above and call quick_attention_analysis(your_sentence)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361f7e1c",
   "metadata": {},
   "source": [
    "## 9. Conclusion and Next Steps\n",
    "\n",
    "### 🎯 **What We've Accomplished**\n",
    "- ✅ Loaded and configured DistilBERT for attention visualization\n",
    "- ✅ Created custom visualization functions for attention heatmaps\n",
    "- ✅ Used BertViz for interactive attention exploration\n",
    "- ✅ Analyzed attention patterns across different sentence types\n",
    "- ✅ Identified key linguistic patterns captured by attention\n",
    "- ✅ Documented systematic observations about attention behavior\n",
    "\n",
    "### 🔍 **Key Takeaways**\n",
    "1. **Attention is Interpretable**: We can observe meaningful linguistic patterns\n",
    "2. **Layer Specialization**: Different layers capture different types of relationships\n",
    "3. **Head Diversity**: Different attention heads specialize in different phenomena\n",
    "4. **Structural Awareness**: Models implicitly learn syntactic and semantic structures\n",
    "\n",
    "### 🚀 **Possible Extensions**\n",
    "- **Try Different Models**: Compare BERT, RoBERTa, GPT-2 attention patterns\n",
    "- **Multilingual Analysis**: Explore attention in different languages\n",
    "- **Fine-tuned Models**: See how attention changes after task-specific training\n",
    "- **Probing Tasks**: Test if attention correlates with specific linguistic phenomena\n",
    "- **Attention Flow**: Track how information flows through layers\n",
    "- **Attention Rollout**: Combine attention across layers for end-to-end analysis\n",
    "\n",
    "### 📚 **Further Reading**\n",
    "- [BertViz Documentation](https://github.com/jessevig/bertviz)\n",
    "- [Attention is All You Need (Original Transformer Paper)](https://arxiv.org/abs/1706.03762)\n",
    "- [What Does BERT Look At?](https://arxiv.org/abs/1906.04341)\n",
    "- [A Primer on Neural Network Models for Natural Language Processing](https://arxiv.org/abs/1510.00726)\n",
    "\n",
    "---\n",
    "\n",
    "**Happy exploring! 🧠✨**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
